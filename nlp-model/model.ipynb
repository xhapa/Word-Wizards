{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tag import hmm\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_incr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = 'If Anaconda (conda) and Jupyter Notebook (Jupyter Lab) are set up the right way the combination of them can become the perfect team, where you are able to easily switch between Deep Learning conda environments.'\n",
    "text_tokens_en = word_tokenize(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_es = 'Con los niveles del mar en aumento, la contaminación por plásticos y la sobrexplotación pesquera, el emergente internet de las cosas submarinas ampliará enormemente los conocimientos sobre los mares del mundo'\n",
    "text_tokens_es = word_tokenize(text_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_file(file, tagtype):\n",
    "    sent_list = []\n",
    "\n",
    "    for token_list in parse_incr(file):\n",
    "        word_list = []\n",
    "        for token in token_list:\n",
    "            word_list.append((token['form'].lower(), token[tagtype]))\n",
    "        sent_list.append(word_list)\n",
    "    \n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('según', 'ADP'),\n",
       " ('el', 'DET'),\n",
       " ('informe', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('el', 'DET')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = open('./datasets/UD_Spanish-AnCora/es_ancora-ud-train.conllu', encoding='utf-8')\n",
    "tagtype = 'upos'\n",
    "data = parse_data_file(data_file, tagtype)\n",
    "data[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9354090758529314"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger_es = hmm.HiddenMarkovModelTagger.train(train_set)\n",
    "predicted_set = tagger_es.tag(text_tokens_es)\n",
    "tagger_es.accuracy(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('If', 'IN'),\n",
       " ('Anaconda', 'NNP'),\n",
       " ('(', '('),\n",
       " ('conda', 'NN'),\n",
       " (')', ')'),\n",
       " ('and', 'CC'),\n",
       " ('Jupyter', 'NNP'),\n",
       " ('Notebook', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Jupyter', 'NNP'),\n",
       " ('Lab', 'NNP'),\n",
       " (')', ')'),\n",
       " ('are', 'VBP'),\n",
       " ('set', 'VBN'),\n",
       " ('up', 'RP'),\n",
       " ('the', 'DT'),\n",
       " ('right', 'JJ'),\n",
       " ('way', 'NN'),\n",
       " ('the', 'DT'),\n",
       " ('combination', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('them', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('become', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('perfect', 'JJ'),\n",
       " ('team', 'NN'),\n",
       " (',', ','),\n",
       " ('where', 'WRB'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('easily', 'RB'),\n",
       " ('switch', 'VB'),\n",
       " ('between', 'IN'),\n",
       " ('Deep', 'NNP'),\n",
       " ('Learning', 'NNP'),\n",
       " ('conda', 'NN'),\n",
       " ('environments', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(text_tokens_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export tagger_es model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hmm_tagger_es.dill', 'wb') as file:\n",
    "    dill.dump(tagger_es, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Con', 'ADP'),\n",
       " ('los', 'DET'),\n",
       " ('niveles', 'NOUN'),\n",
       " ('del', '_'),\n",
       " ('mar', 'VERB'),\n",
       " ('en', 'ADP'),\n",
       " ('aumento', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('la', 'DET'),\n",
       " ('contaminación', 'NOUN'),\n",
       " ('por', 'ADP'),\n",
       " ('plásticos', 'NOUN'),\n",
       " ('y', 'CCONJ'),\n",
       " ('la', 'DET'),\n",
       " ('sobrexplotación', 'NOUN'),\n",
       " ('pesquera', 'ADJ'),\n",
       " (',', 'PUNCT'),\n",
       " ('el', 'DET'),\n",
       " ('emergente', 'NUM'),\n",
       " ('internet', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('las', 'DET'),\n",
       " ('cosas', 'NOUN'),\n",
       " ('submarinas', 'ADJ'),\n",
       " ('ampliará', 'PUNCT'),\n",
       " ('enormemente', 'ADV'),\n",
       " ('los', 'DET'),\n",
       " ('conocimientos', 'NOUN'),\n",
       " ('sobre', 'ADP'),\n",
       " ('los', 'DET'),\n",
       " ('mares', 'NOUN'),\n",
       " ('del', '_'),\n",
       " ('mundo', 'ADP')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('hmm_tagger_es.dill', 'rb') as file:\n",
    "    loaded_tagger = dill.load(file)\n",
    "\n",
    "loaded_tagger.tag(text_tokens_es)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKPreProcessing():\n",
    "    def __init__(self, text, lang) -> None:\n",
    "        self.text = text  \n",
    "        self.lang = lang\n",
    "\n",
    "    def remove_html_tags(self):\n",
    "        return self\n",
    "    \n",
    "    def to_lower(self):\n",
    "        self.text = self.text.lower()\n",
    "        return self\n",
    "\n",
    "    def remove_double_spaces(self):\n",
    "        self.text = [words for words in self.text if re.sub(' +', ' ', words)]\n",
    "        return self\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.text = word_tokenize(self.text)\n",
    "        return self\n",
    "    \n",
    "    def get_preprocessed(self):\n",
    "        return self.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class NLTKTextPreprocessor(TransformerMixin, BaseEstimator):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, text):\n",
    "    return self\n",
    "\n",
    "  def transform(self, text):\n",
    "    txt_preproc = NLTKPreProcessing(text, 'es')\n",
    "    processed_text = \\\n",
    "        txt_preproc \\\n",
    "        .remove_html_tags()\\\n",
    "        .to_lower()\\\n",
    "        .tokenize()\\\n",
    "        .remove_double_spaces()\\\n",
    "        .get_preprocessed()\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pure_transformation_pipeline = Pipeline(steps=[\n",
    "           ('text_preproc', NLTKTextPreprocessor())])\n",
    "tfidf_data = pure_transformation_pipeline.fit_transform(text_es)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastAPI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
