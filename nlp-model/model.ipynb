{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tag import hmm\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_incr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = 'Finally, we demonstrated lemmatization on a list of words, specifying n as the part-of-speech tag for all the words in the list. This resulted in the base form (lemma) of each word.'\n",
    "text_tokens_en = word_tokenize(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_es = 'Con los niveles del mar en aumento, la contaminación por plásticos y la sobrexplotación pesquera, el emergente internet de las cosas submarinas ampliará enormemente los conocimientos sobre los mares del mundo'\n",
    "text_tokens_es = word_tokenize(text_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_file(file, tagtype):\n",
    "    sent_list = []\n",
    "\n",
    "    for token_list in parse_incr(file):\n",
    "        word_list = []\n",
    "        for token in token_list:\n",
    "            word_list.append((token['form'].lower(), token[tagtype]))\n",
    "        sent_list.append(word_list)\n",
    "    \n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('según', 'ADP'),\n",
       " ('el', 'DET'),\n",
       " ('informe', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('el', 'DET')]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = open('./datasets/UD_Spanish-AnCora/es_ancora-ud-train.conllu', encoding='utf-8')\n",
    "tagtype = 'upos'\n",
    "data = parse_data_file(data_file, tagtype)\n",
    "data[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9347736491616234"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger_es = hmm.HiddenMarkovModelTagger.train(train_set)\n",
    "predicted_set = tagger_es.tag(text_tokens_es)\n",
    "tagger_es.accuracy(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Finally', 'RB'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('demonstrated', 'VBD'),\n",
       " ('lemmatization', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('list', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('words', 'NNS'),\n",
       " (',', ','),\n",
       " ('specifying', 'VBG'),\n",
       " ('n', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('part-of-speech', 'JJ'),\n",
       " ('tag', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('all', 'PDT'),\n",
       " ('the', 'DT'),\n",
       " ('words', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('list', 'NN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('resulted', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('base', 'NN'),\n",
       " ('form', 'NN'),\n",
       " ('(', '('),\n",
       " ('lemma', 'JJ'),\n",
       " (')', ')'),\n",
       " ('of', 'IN'),\n",
       " ('each', 'DT'),\n",
       " ('word', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(text_tokens_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export tagger_es model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hmm_tagger_es.dill', 'wb') as file:\n",
    "    dill.dump(tagger_es, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Con', 'ADP'),\n",
       " ('los', 'DET'),\n",
       " ('niveles', 'NOUN'),\n",
       " ('del', '_'),\n",
       " ('mar', 'VERB'),\n",
       " ('en', 'ADP'),\n",
       " ('aumento', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('la', 'DET'),\n",
       " ('contaminación', 'NOUN'),\n",
       " ('por', 'ADP'),\n",
       " ('plásticos', 'PROPN'),\n",
       " ('y', 'CCONJ'),\n",
       " ('la', 'DET'),\n",
       " ('sobrexplotación', 'NOUN'),\n",
       " ('pesquera', 'ADJ'),\n",
       " (',', 'PUNCT'),\n",
       " ('el', 'DET'),\n",
       " ('emergente', 'ADJ'),\n",
       " ('internet', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('las', 'DET'),\n",
       " ('cosas', 'NOUN'),\n",
       " ('submarinas', 'AUX'),\n",
       " ('ampliará', 'VERB'),\n",
       " ('enormemente', 'ADV'),\n",
       " ('los', 'DET'),\n",
       " ('conocimientos', 'NOUN'),\n",
       " ('sobre', 'ADP'),\n",
       " ('los', 'DET'),\n",
       " ('mares', 'NOUN'),\n",
       " ('del', '_'),\n",
       " ('mundo', 'ADP')]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('hmm_tagger_es.dill', 'rb') as file:\n",
    "    loaded_tagger = dill.load(file)\n",
    "\n",
    "loaded_tagger.tag(text_tokens_es)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKPreProcessing():\n",
    "    def __init__(self, text, lang) -> None:\n",
    "        self.text = text  \n",
    "        self.lang = lang\n",
    "\n",
    "    def remove_html_tags(self):\n",
    "        return self\n",
    "    \n",
    "    def to_lower(self):\n",
    "        self.text = self.text.lower()\n",
    "        return self\n",
    "\n",
    "    def remove_double_spaces(self):\n",
    "        self.text = [words for words in self.text if re.sub(' +', ' ', words)]\n",
    "        return self\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.text = word_tokenize(self.text)\n",
    "        return self\n",
    "    \n",
    "    def pos(self):\n",
    "        if self.lang == 'es':\n",
    "            self.text =  tagger_es.tag(self.text)\n",
    "        elif self.lang == 'en':\n",
    "            self.text =  pos_tag(self.text)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        if self.lang == 'es':\n",
    "            self.stopwd = stopwords.words('spanish')\n",
    "        elif self.lang == 'en':\n",
    "            self.stopwd = stopwords.words('english')\n",
    "        \n",
    "        self.text = [(word, tag) for word, tag in self.text if word not in self.stopwd]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_preprocessed(self):\n",
    "        return self.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class NLTKTextPreprocessor(TransformerMixin, BaseEstimator):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, text):\n",
    "    return self\n",
    "\n",
    "  def transform(self, text):\n",
    "    txt_preproc = NLTKPreProcessing(text, 'es')\n",
    "    processed_text = \\\n",
    "        txt_preproc \\\n",
    "        .remove_html_tags()\\\n",
    "        .to_lower()\\\n",
    "        .tokenize()\\\n",
    "        .remove_double_spaces()\\\n",
    "        .pos()\\\n",
    "        .remove_stopwords()\\\n",
    "        .get_preprocessed()\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('niveles', 'NOUN'),\n",
       " ('mar', 'VERB'),\n",
       " ('aumento', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('contaminación', 'NOUN'),\n",
       " ('plásticos', 'PROPN'),\n",
       " ('sobrexplotación', 'NOUN'),\n",
       " ('pesquera', 'ADJ'),\n",
       " (',', 'PUNCT'),\n",
       " ('emergente', 'ADJ'),\n",
       " ('internet', 'NOUN'),\n",
       " ('cosas', 'NOUN'),\n",
       " ('submarinas', 'AUX'),\n",
       " ('ampliará', 'VERB'),\n",
       " ('enormemente', 'ADV'),\n",
       " ('conocimientos', 'NOUN'),\n",
       " ('mares', 'NOUN'),\n",
       " ('mundo', 'ADP')]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pure_transformation_pipeline = Pipeline(steps=[\n",
    "           ('text_preproc', NLTKTextPreprocessor())])\n",
    "tfidf_data = pure_transformation_pipeline.fit_transform(text_es)\n",
    "tfidf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 10.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from es-core-news-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.49.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKLemmatization():\n",
    "    def __init__(self, text, lang) -> None:\n",
    "        self.text = text\n",
    "        self.lang = lang\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    def get_wordnet_pos(self, tag):\n",
    "        tag = tag[0].upper()\n",
    "        tag_dict = {\n",
    "            'J': 'a',  # Adjective\n",
    "            'V': 'v',  # Verb\n",
    "            'N': 'n',  # Noun\n",
    "            'R': 'r'   # Adverb\n",
    "        }\n",
    "        return tag_dict.get(tag, 'n')\n",
    "    \n",
    "\n",
    "    def get_wordnet_upos(self, upos_tag):\n",
    "        tag_dict = {\n",
    "            'NOUN': 'n',  # Noun\n",
    "            'VERB': 'v',  # Verb\n",
    "            'ADJ': 'a',   # Adjective\n",
    "            'ADV': 'r'    # Adverb\n",
    "        }\n",
    "        return tag_dict.get(upos_tag, 'n')  \n",
    "\n",
    "    def lemmatize(self):\n",
    "        if self.lang == 'en':\n",
    "            self.text = [self.lemmatizer.lemmatize(word, pos=self.get_wordnet_pos(tag)) for word, tag in self.text]\n",
    "        elif self.lang == 'es':\n",
    "            words_with_space = [word + ' ' for word, tag in self.text]\n",
    "            doc = self.nlp(''.join(words_with_space))\n",
    "            self.text = [token.lemma_ for token in doc]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def get_lemmatization(self):\n",
    "        return  self.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKLemmatizationProcessor(TransformerMixin, BaseEstimator):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, text):\n",
    "    return self\n",
    "\n",
    "  def transform(self, text):\n",
    "    txt_lemma = NLTKLemmatization(text, 'es')\n",
    "    processed_text = \\\n",
    "        txt_lemma \\\n",
    "        .lemmatize()\\\n",
    "        .get_lemmatization()\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nivel',\n",
       " 'mar',\n",
       " 'aumento',\n",
       " ',',\n",
       " 'contaminación',\n",
       " 'plástico',\n",
       " 'sobrexplotación',\n",
       " 'pesquero',\n",
       " ',',\n",
       " 'emergente',\n",
       " 'internet',\n",
       " 'cosa',\n",
       " 'submarino',\n",
       " 'ampliar',\n",
       " 'enormemente',\n",
       " 'conocimiento',\n",
       " 'mar',\n",
       " 'mundo']"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization_pipeline = Pipeline(steps=[\n",
    "           ('text_lemma', NLTKLemmatizationProcessor())])\n",
    "data = lemmatization_pipeline.fit_transform(tfidf_data)\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastAPI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
