{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/xhapa/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk import word_tokenize, pos_tag, sent_tokenize, ne_chunk\n",
    "from nltk.tag import hmm\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_incr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = 'Elon Musk said the incident Wednesday when Senate Minority Leader Mitch McConnell, R-Ky., appeared to freeze up during a news conference with other Republican leaders should warrant a \"need\" for a constitutional amendment.Musk, who owns Twitter and other companies, tweeted early Thursday morning, \"We need a constitutional amendment. This is insane.\"The comment came in response to another user who provided a video of the incident showing McConnell addressing a group of reporters before abruptly stopping, staring blankly into the group, and standing idle for a brief moment. A Republican colleague broke McConnell’s apparent trance and helped escort him away from the podium.Musk’s tweet did not provide specifics on what the potential amendment would say or do. Several people who responded suggested there should be age limits for members of Congress.'\n",
    "text_tokens_en = word_tokenize(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_es = 'El ADN desvela el árbol genealógico más antiguo de una familia, hace 6.700 años El estudio genético de casi 100 cadáveres en una necrópolis de Francia reconstruye siete generaciones de un clan del Neolítico en el que los hombres se quedaban toda la vida en su lugar de origen mientras las mujeres se iban a otros grupos Entre 2004 y 2007, un grupo de arqueólogos excavó en Francia un cementerio de hace 6.700 años donde aparecieron más de 100 cadáveres de todas las edades. Las tumbas eran individuales y se habían excavado con cuidado para que ninguna quedase encima de otra. Apenas había objetos de valor junto a los muertos, algo raro, pues en aquella época estaba triunfando el Neolítico, la revolución que trajo a Europa el sedentarismo, la agricultura y la desigualdad. Por primera vez en la historia se pudo acumular grandes cantidades de alimento, y las primeras riquezas. Por razones desconocidas, grupos cada vez más grandes comenzaron a reunirse para levantar espectaculares monumentos megalíticos y tumbas donde se enterraba a las élites junto a objetos valiosos o sagrados, como armas y animales. En cambio, el cementerio francés parecía el de la gente corriente del momento. Ahora, un grupo de científicos francoalemanes ha conseguido extraer ADN de 94 cadáveres del cementerio francés para obtener su genoma completo. Las secuencias genéticas han dibujado los lazos de parentesco entre los fallecidos hasta componer un árbol de familia que se remonta siete generaciones; el mayor y más antiguo que se conoce de nuestra especie. Los resultados se publican hoy en Nature, referente de la mejor ciencia mundial. Los investigadores han encontrado que en este cementerio de Gurgy Les Noisats, al sur de París, hay dos grandes clanes familiares, encabezados por dos hombres. El más grande comprende a 64 familiares y abarca siete generaciones, todos enterrados en el mismo lugar. El segundo grupo lo forman 12 familiares de cinco generaciones. Apenas se ha descubierto un cruce entre los dos grupos: una de las madres del clan pequeño era pariente de un hombre del grande. El ADN de este colectivo que vivió hace casi siete milenios abre una ventana única para entender la familia, la sociedad y la cultura de una época tan importante como desconocida, pues aún no había escritura. El trabajo muestra un fenómeno clarísimo: los hombres de la familia se quedaban en su lugar de nacimiento para toda la vida, mientras las mujeres dejaban el seno familiar para ir a vivir con otros grupos. Los isótopos de estroncio acumulados en los dientes indican de dónde proviene el agua que bebió una persona durante su niñez, y los de las mujeres enterradas en Gurgy son de muchos lugares distintos. En cambio, apenas hay mujeres que fueran parte de los dos clanes originales del lugar. Estos hallazgos refuerzan una tendencia observada en otros yacimientos neolíticos posteriores: los hombres se quedaban y las mujeres se marchaban a vivir y formar familias en otros grupos, una práctica común en humanos y otros primates conocida como patrilocalidad y que evita los problemas asociados a la endogamia. La paleogenetista Maïte Rivollat, primera autora del estudio, destaca otro hallazgo sorprendente: “Hemos encontrado parejas que tuvieron muchos hijos. En un caso vemos hasta seis hermanos que vivieron hasta la edad adulta, y a su vez tuvieron varios hijos, lo que supone una familia muy extensa. Probablemente, tuvieron también hermanas cuyos restos no están aquí, pues se fueron a vivir a otros grupos”. Para su equipo, esto indica una gran fertilidad de las mujeres e implica que había abundancia de alimentos y probablemente estabilidad social. De hecho, no hay ni un signo de violencia en ninguno de los más de 100 cadáveres del cementerio. Apenas se han encontrado medio hermanos en ninguno de los dos clanes familiares. Tampoco se observa que los viudos y las viudas se emparejasen con sus cuñados. Esto implica que las parejas eran monógamas y que ya había una idea clara de evitar tener hijos con parientes cercanos.'\n",
    "text_tokens_es = word_tokenize(text_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_file(file, tagtype):\n",
    "    sent_list = []\n",
    "\n",
    "    for token_list in parse_incr(file):\n",
    "        word_list = []\n",
    "        for token in token_list:\n",
    "            word_list.append((token['form'].lower(), token[tagtype]))\n",
    "        sent_list.append(word_list)\n",
    "    \n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('según', 'ADP'),\n",
       " ('el', 'DET'),\n",
       " ('informe', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('el', 'DET')]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = open('./datasets/UD_Spanish-AnCora/es_ancora-ud-train.conllu', encoding='utf-8')\n",
    "tagtype = 'upos'\n",
    "data = parse_data_file(data_file, tagtype)\n",
    "data[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9343390850423671"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger_es = hmm.HiddenMarkovModelTagger.train(train_set)\n",
    "predicted_set = tagger_es.tag(text_tokens_es)\n",
    "tagger_es.accuracy(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Elon', 'NNP'),\n",
       " ('Musk', 'NNP'),\n",
       " ('said', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('incident', 'NN'),\n",
       " ('Wednesday', 'NNP'),\n",
       " ('when', 'WRB'),\n",
       " ('Senate', 'NNP'),\n",
       " ('Minority', 'NNP'),\n",
       " ('Leader', 'NNP'),\n",
       " ('Mitch', 'NNP'),\n",
       " ('McConnell', 'NNP'),\n",
       " (',', ','),\n",
       " ('R-Ky.', 'NNP'),\n",
       " (',', ','),\n",
       " ('appeared', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('freeze', 'VB'),\n",
       " ('up', 'RP'),\n",
       " ('during', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('news', 'NN'),\n",
       " ('conference', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('other', 'JJ'),\n",
       " ('Republican', 'JJ'),\n",
       " ('leaders', 'NNS'),\n",
       " ('should', 'MD'),\n",
       " ('warrant', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('``', '``'),\n",
       " ('need', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('for', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('constitutional', 'JJ'),\n",
       " ('amendment.Musk', 'NN'),\n",
       " (',', ','),\n",
       " ('who', 'WP'),\n",
       " ('owns', 'VBZ'),\n",
       " ('Twitter', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('other', 'JJ'),\n",
       " ('companies', 'NNS'),\n",
       " (',', ','),\n",
       " ('tweeted', 'VBD'),\n",
       " ('early', 'JJ'),\n",
       " ('Thursday', 'NNP'),\n",
       " ('morning', 'NN'),\n",
       " (',', ','),\n",
       " ('``', '``'),\n",
       " ('We', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('constitutional', 'JJ'),\n",
       " ('amendment', 'NN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('insane', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('``', '``'),\n",
       " ('The', 'DT'),\n",
       " ('comment', 'NN'),\n",
       " ('came', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('response', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('another', 'DT'),\n",
       " ('user', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('provided', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('video', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('incident', 'NN'),\n",
       " ('showing', 'VBG'),\n",
       " ('McConnell', 'NNP'),\n",
       " ('addressing', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('group', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('reporters', 'NNS'),\n",
       " ('before', 'IN'),\n",
       " ('abruptly', 'RB'),\n",
       " ('stopping', 'VBG'),\n",
       " (',', ','),\n",
       " ('staring', 'VBG'),\n",
       " ('blankly', 'RB'),\n",
       " ('into', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('group', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('standing', 'VBG'),\n",
       " ('idle', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('brief', 'JJ'),\n",
       " ('moment', 'NN'),\n",
       " ('.', '.'),\n",
       " ('A', 'DT'),\n",
       " ('Republican', 'JJ'),\n",
       " ('colleague', 'NN'),\n",
       " ('broke', 'VBD'),\n",
       " ('McConnell', 'NNP'),\n",
       " ('’', 'NNP'),\n",
       " ('s', 'VBD'),\n",
       " ('apparent', 'JJ'),\n",
       " ('trance', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('helped', 'VBD'),\n",
       " ('escort', 'VB'),\n",
       " ('him', 'PRP'),\n",
       " ('away', 'RB'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('podium.Musk', 'NN'),\n",
       " ('’', 'NNP'),\n",
       " ('s', 'VBD'),\n",
       " ('tweet', 'NN'),\n",
       " ('did', 'VBD'),\n",
       " ('not', 'RB'),\n",
       " ('provide', 'VB'),\n",
       " ('specifics', 'NNS'),\n",
       " ('on', 'IN'),\n",
       " ('what', 'WP'),\n",
       " ('the', 'DT'),\n",
       " ('potential', 'JJ'),\n",
       " ('amendment', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('say', 'VB'),\n",
       " ('or', 'CC'),\n",
       " ('do', 'VB'),\n",
       " ('.', '.'),\n",
       " ('Several', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('who', 'WP'),\n",
       " ('responded', 'VBD'),\n",
       " ('suggested', 'VBN'),\n",
       " ('there', 'EX'),\n",
       " ('should', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('age', 'NN'),\n",
       " ('limits', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('members', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('Congress', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(text_tokens_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export tagger_es model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hmm_tagger_es.dill', 'wb') as file:\n",
    "    dill.dump(tagger_es, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('El', 'PRON'),\n",
       " ('ADN', 'AUX'),\n",
       " ('desvela', 'VERB'),\n",
       " ('el', 'DET'),\n",
       " ('árbol', 'NOUN'),\n",
       " ('genealógico', 'PUNCT'),\n",
       " ('más', 'ADV'),\n",
       " ('antiguo', 'ADJ'),\n",
       " ('de', 'ADP'),\n",
       " ('una', 'DET'),\n",
       " ('familia', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('hace', 'VERB'),\n",
       " ('6.700', 'NUM'),\n",
       " ('años', 'NOUN'),\n",
       " ('El', 'ADJ'),\n",
       " ('estudio', 'NOUN'),\n",
       " ('genético', 'ADJ'),\n",
       " ('de', 'ADP'),\n",
       " ('casi', 'ADV'),\n",
       " ('100', 'NUM'),\n",
       " ('cadáveres', 'NOUN'),\n",
       " ('en', 'ADP'),\n",
       " ('una', 'DET'),\n",
       " ('necrópolis', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('Francia', 'NOUN'),\n",
       " ('reconstruye', 'ADP'),\n",
       " ('siete', 'NUM'),\n",
       " ('generaciones', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('un', 'DET'),\n",
       " ('clan', 'NOUN'),\n",
       " ('del', '_'),\n",
       " ('Neolítico', 'VERB'),\n",
       " ('en', 'ADP'),\n",
       " ('el', 'DET'),\n",
       " ('que', 'PRON'),\n",
       " ('los', 'DET'),\n",
       " ('hombres', 'NOUN'),\n",
       " ('se', 'PRON'),\n",
       " ('quedaban', 'VERB'),\n",
       " ('toda', 'DET'),\n",
       " ('la', 'DET'),\n",
       " ('vida', 'NOUN'),\n",
       " ('en', 'ADP'),\n",
       " ('su', 'DET'),\n",
       " ('lugar', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('origen', 'NOUN'),\n",
       " ('mientras', 'CCONJ'),\n",
       " ('las', 'DET'),\n",
       " ('mujeres', 'NOUN'),\n",
       " ('se', 'PRON'),\n",
       " ('iban', 'VERB'),\n",
       " ('a', 'ADP'),\n",
       " ('otros', 'DET'),\n",
       " ('grupos', 'NOUN'),\n",
       " ('Entre', 'ADJ'),\n",
       " ('2004', 'NOUN'),\n",
       " ('y', 'CCONJ'),\n",
       " ('2007', 'NUM'),\n",
       " (',', 'PUNCT'),\n",
       " ('un', 'DET'),\n",
       " ('grupo', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('arqueólogos', 'NOUN'),\n",
       " ('excavó', '_'),\n",
       " ('en', 'ADP'),\n",
       " ('Francia', 'SCONJ'),\n",
       " ('un', 'DET'),\n",
       " ('cementerio', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('hace', 'VERB'),\n",
       " ('6.700', 'NUM'),\n",
       " ('años', 'NOUN'),\n",
       " ('donde', 'ADV'),\n",
       " ('aparecieron', 'VERB'),\n",
       " ('más', 'ADV'),\n",
       " ('de', 'ADP'),\n",
       " ('100', 'NUM'),\n",
       " ('cadáveres', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('todas', 'DET'),\n",
       " ('las', 'DET'),\n",
       " ('edades', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Las', 'SCONJ'),\n",
       " ('tumbas', 'PRON'),\n",
       " ('eran', 'AUX'),\n",
       " ('individuales', 'ADJ'),\n",
       " ('y', 'CCONJ'),\n",
       " ('se', 'PRON'),\n",
       " ('habían', 'AUX'),\n",
       " ('excavado', 'VERB'),\n",
       " ('con', 'ADP'),\n",
       " ('cuidado', 'NOUN'),\n",
       " ('para', 'ADP'),\n",
       " ('que', 'SCONJ'),\n",
       " ('ninguna', 'PRON'),\n",
       " ('quedase', 'VERB'),\n",
       " ('encima', 'ADV'),\n",
       " ('de', 'ADP'),\n",
       " ('otra', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Apenas', 'PRON'),\n",
       " ('había', 'AUX'),\n",
       " ('objetos', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('valor', 'NOUN'),\n",
       " ('junto', 'ADJ'),\n",
       " ('a', 'ADP'),\n",
       " ('los', 'DET'),\n",
       " ('muertos', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('algo', 'ADV'),\n",
       " ('raro', 'ADJ'),\n",
       " (',', 'PUNCT'),\n",
       " ('pues', 'SCONJ'),\n",
       " ('en', 'ADP'),\n",
       " ('aquella', 'DET'),\n",
       " ('época', 'NOUN'),\n",
       " ('estaba', 'AUX'),\n",
       " ('triunfando', 'VERB'),\n",
       " ('el', 'DET'),\n",
       " ('Neolítico', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('la', 'DET'),\n",
       " ('revolución', 'NOUN'),\n",
       " ('que', 'PRON'),\n",
       " ('trajo', 'VERB'),\n",
       " ('a', 'ADP'),\n",
       " ('Europa', 'SCONJ'),\n",
       " ('el', 'DET'),\n",
       " ('sedentarismo', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('la', 'DET'),\n",
       " ('agricultura', 'NOUN'),\n",
       " ('y', 'CCONJ'),\n",
       " ('la', 'DET'),\n",
       " ('desigualdad', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Por', 'ADV'),\n",
       " ('primera', 'ADJ'),\n",
       " ('vez', 'NOUN'),\n",
       " ('en', 'ADP'),\n",
       " ('la', 'DET'),\n",
       " ('historia', 'NOUN'),\n",
       " ('se', 'PRON'),\n",
       " ('pudo', 'AUX'),\n",
       " ('acumular', 'VERB'),\n",
       " ('grandes', 'ADJ'),\n",
       " ('cantidades', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('alimento', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('y', 'CCONJ'),\n",
       " ('las', 'DET'),\n",
       " ('primeras', 'ADJ'),\n",
       " ('riquezas', 'PUNCT'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Por', 'DET'),\n",
       " ('razones', 'NOUN'),\n",
       " ('desconocidas', 'ADJ'),\n",
       " (',', 'PUNCT'),\n",
       " ('grupos', 'NOUN'),\n",
       " ('cada', 'DET'),\n",
       " ('vez', 'NOUN'),\n",
       " ('más', 'ADV'),\n",
       " ('grandes', 'ADJ'),\n",
       " ('comenzaron', 'VERB'),\n",
       " ('a', 'ADP'),\n",
       " ('reunirse', '_'),\n",
       " ('para', 'ADP'),\n",
       " ('levantar', 'VERB'),\n",
       " ('espectaculares', 'ADJ'),\n",
       " ('monumentos', 'NOUN'),\n",
       " ('megalíticos', 'ADJ'),\n",
       " ('y', 'CCONJ'),\n",
       " ('tumbas', 'PRON'),\n",
       " ('donde', 'ADV'),\n",
       " ('se', 'PRON'),\n",
       " ('enterraba', 'VERB'),\n",
       " ('a', 'ADP'),\n",
       " ('las', 'DET'),\n",
       " ('élites', 'NOUN'),\n",
       " ('junto', 'ADJ'),\n",
       " ('a', 'ADP'),\n",
       " ('objetos', 'NOUN'),\n",
       " ('valiosos', 'ADJ'),\n",
       " ('o', 'CCONJ'),\n",
       " ('sagrados', 'ADJ'),\n",
       " (',', 'PUNCT'),\n",
       " ('como', 'SCONJ'),\n",
       " ('armas', 'NOUN'),\n",
       " ('y', 'CCONJ'),\n",
       " ('animales', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('En', 'DET'),\n",
       " ('cambio', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('el', 'DET'),\n",
       " ('cementerio', 'NOUN'),\n",
       " ('francés', 'ADJ'),\n",
       " ('parecía', 'VERB'),\n",
       " ('el', 'DET'),\n",
       " ('de', 'ADP'),\n",
       " ('la', 'DET'),\n",
       " ('gente', 'NOUN'),\n",
       " ('corriente', 'ADJ'),\n",
       " ('del', 'ADP'),\n",
       " ('momento', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Ahora', 'PROPN'),\n",
       " (',', 'PUNCT'),\n",
       " ('un', 'DET'),\n",
       " ('grupo', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('científicos', 'NOUN'),\n",
       " ('francoalemanes', 'PRON'),\n",
       " ('ha', 'AUX'),\n",
       " ('conseguido', 'VERB'),\n",
       " ('extraer', 'VERB'),\n",
       " ('ADN', '_'),\n",
       " ('de', 'ADP'),\n",
       " ('94', 'NUM'),\n",
       " ('cadáveres', 'NOUN'),\n",
       " ('del', '_'),\n",
       " ('cementerio', 'ADP'),\n",
       " ('francés', 'NOUN'),\n",
       " ('para', 'ADP'),\n",
       " ('obtener', 'VERB'),\n",
       " ('su', 'DET'),\n",
       " ('genoma', 'NOUN'),\n",
       " ('completo', 'ADJ'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Las', 'DET'),\n",
       " ('secuencias', 'NOUN'),\n",
       " ('genéticas', 'ADJ'),\n",
       " ('han', 'AUX'),\n",
       " ('dibujado', 'VERB'),\n",
       " ('los', 'DET'),\n",
       " ('lazos', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('parentesco', 'NOUN'),\n",
       " ('entre', 'ADP'),\n",
       " ('los', 'DET'),\n",
       " ('fallecidos', 'ADJ'),\n",
       " ('hasta', 'ADP'),\n",
       " ('componer', 'VERB'),\n",
       " ('un', 'DET'),\n",
       " ('árbol', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('familia', 'NOUN'),\n",
       " ('que', 'PRON'),\n",
       " ('se', 'PRON'),\n",
       " ('remonta', 'VERB'),\n",
       " ('siete', 'NUM'),\n",
       " ('generaciones', 'NOUN'),\n",
       " (';', 'PUNCT'),\n",
       " ('el', 'DET'),\n",
       " ('mayor', 'ADJ'),\n",
       " ('y', 'CCONJ'),\n",
       " ('más', 'ADV'),\n",
       " ('antiguo', 'ADJ'),\n",
       " ('que', 'SCONJ'),\n",
       " ('se', 'PRON'),\n",
       " ('conoce', 'VERB'),\n",
       " ('de', 'ADP'),\n",
       " ('nuestra', 'DET'),\n",
       " ('especie', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Los', 'DET'),\n",
       " ('resultados', 'NOUN'),\n",
       " ('se', 'PRON'),\n",
       " ('publican', 'VERB'),\n",
       " ('hoy', 'ADV'),\n",
       " ('en', 'ADP'),\n",
       " ('Nature', 'PROPN'),\n",
       " (',', 'PUNCT'),\n",
       " ('referente', 'ADJ'),\n",
       " ('de', 'ADP'),\n",
       " ('la', 'DET'),\n",
       " ('mejor', 'ADJ'),\n",
       " ('ciencia', 'NOUN'),\n",
       " ('mundial', 'ADJ'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Los', 'DET'),\n",
       " ('investigadores', 'NOUN'),\n",
       " ('han', 'AUX'),\n",
       " ('encontrado', 'VERB'),\n",
       " ('que', 'SCONJ'),\n",
       " ('en', 'ADP'),\n",
       " ('este', 'DET'),\n",
       " ('cementerio', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('Gurgy', 'DET'),\n",
       " ('Les', 'NOUN'),\n",
       " ('Noisats', 'ADJ'),\n",
       " (',', 'PUNCT'),\n",
       " ('al', 'DET'),\n",
       " ('sur', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('París', 'PROPN'),\n",
       " (',', 'PUNCT'),\n",
       " ('hay', 'VERB'),\n",
       " ('dos', 'NUM'),\n",
       " ('grandes', 'ADJ'),\n",
       " ('clanes', 'ADP'),\n",
       " ('familiares', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('encabezados', '_'),\n",
       " ('por', 'ADP'),\n",
       " ('dos', 'NUM'),\n",
       " ('hombres', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('El', 'CCONJ'),\n",
       " ('más', 'ADV'),\n",
       " ('grande', 'ADJ'),\n",
       " ('comprende', 'VERB'),\n",
       " ('a', 'ADP'),\n",
       " ('64', 'NUM'),\n",
       " ('familiares', 'NOUN'),\n",
       " ('y', 'CCONJ'),\n",
       " ('abarca', 'VERB'),\n",
       " ('siete', 'NUM'),\n",
       " ('generaciones', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('todos', 'PRON'),\n",
       " ('enterrados', 'VERB'),\n",
       " ('en', 'ADP'),\n",
       " ('el', 'DET'),\n",
       " ('mismo', 'ADJ'),\n",
       " ('lugar', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('El', 'ADV'),\n",
       " ('segundo', 'ADJ'),\n",
       " ('grupo', 'NOUN'),\n",
       " ('lo', 'PRON'),\n",
       " ('forman', 'VERB'),\n",
       " ('12', 'NUM'),\n",
       " ('familiares', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('cinco', 'NUM'),\n",
       " ('generaciones', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Apenas', 'SCONJ'),\n",
       " ('se', 'PRON'),\n",
       " ('ha', 'AUX'),\n",
       " ('descubierto', 'VERB'),\n",
       " ('un', 'DET'),\n",
       " ('cruce', 'NOUN'),\n",
       " ('entre', 'ADP'),\n",
       " ('los', 'DET'),\n",
       " ('dos', 'NUM'),\n",
       " ('grupos', 'NOUN'),\n",
       " (':', 'PUNCT'),\n",
       " ('una', 'PRON'),\n",
       " ('de', 'ADP'),\n",
       " ('las', 'DET'),\n",
       " ('madres', 'NOUN'),\n",
       " ('del', '_'),\n",
       " ('clan', 'ADP'),\n",
       " ('pequeño', 'ADJ'),\n",
       " ('era', 'AUX'),\n",
       " ('pariente', 'ADJ'),\n",
       " ('de', 'ADP'),\n",
       " ('un', 'DET'),\n",
       " ('hombre', 'NOUN'),\n",
       " ('del', '_'),\n",
       " ('grande', 'VERB'),\n",
       " ('.', 'PUNCT'),\n",
       " ('El', 'PRON'),\n",
       " ('ADN', 'VERB'),\n",
       " ('de', 'ADP'),\n",
       " ('este', 'DET'),\n",
       " ('colectivo', 'NOUN'),\n",
       " ('que', 'PRON'),\n",
       " ('vivió', 'VERB'),\n",
       " ('hace', 'VERB'),\n",
       " ('casi', 'ADV'),\n",
       " ('siete', 'NUM'),\n",
       " ('milenios', 'NOUN'),\n",
       " ('abre', 'VERB'),\n",
       " ('una', 'DET'),\n",
       " ('ventana', 'NOUN'),\n",
       " ('única', 'ADJ'),\n",
       " ('para', 'ADP'),\n",
       " ('entender', 'VERB'),\n",
       " ('la', 'DET'),\n",
       " ('familia', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('la', 'DET'),\n",
       " ('sociedad', 'NOUN'),\n",
       " ('y', 'CCONJ'),\n",
       " ('la', 'DET'),\n",
       " ('cultura', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('una', 'DET'),\n",
       " ('época', 'NOUN'),\n",
       " ('tan', 'ADV'),\n",
       " ('importante', 'ADJ'),\n",
       " ('como', 'SCONJ'),\n",
       " ('desconocida', 'PROPN'),\n",
       " (',', 'PUNCT'),\n",
       " ('pues', 'SCONJ'),\n",
       " ('aún', 'ADV'),\n",
       " ('no', 'ADV'),\n",
       " ('había', 'AUX'),\n",
       " ('escritura', 'ADJ'),\n",
       " ('.', 'PUNCT'),\n",
       " ('El', 'DET'),\n",
       " ('trabajo', 'NOUN'),\n",
       " ('muestra', 'VERB'),\n",
       " ('un', 'DET'),\n",
       " ('fenómeno', 'NOUN'),\n",
       " ('clarísimo', 'ADJ'),\n",
       " (':', 'PUNCT'),\n",
       " ('los', 'DET'),\n",
       " ('hombres', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('la', 'DET'),\n",
       " ('familia', 'NOUN'),\n",
       " ('se', 'PRON'),\n",
       " ('quedaban', 'VERB'),\n",
       " ('en', 'ADP'),\n",
       " ('su', 'DET'),\n",
       " ('lugar', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('nacimiento', 'NOUN'),\n",
       " ('para', 'ADP'),\n",
       " ('toda', 'DET'),\n",
       " ('la', 'DET'),\n",
       " ('vida', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('mientras', 'CCONJ'),\n",
       " ('las', 'DET'),\n",
       " ('mujeres', 'NOUN'),\n",
       " ('dejaban', 'VERB'),\n",
       " ('el', 'DET'),\n",
       " ('seno', 'NOUN'),\n",
       " ('familiar', 'ADJ'),\n",
       " ('para', 'ADP'),\n",
       " ('ir', 'VERB'),\n",
       " ('a', 'ADP'),\n",
       " ('vivir', 'VERB'),\n",
       " ('con', 'ADP'),\n",
       " ('otros', 'DET'),\n",
       " ('grupos', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Los', 'DET'),\n",
       " ('isótopos', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('estroncio', 'DET'),\n",
       " ('acumulados', 'ADJ'),\n",
       " ('en', 'ADP'),\n",
       " ('los', 'DET'),\n",
       " ('dientes', 'NOUN'),\n",
       " ('indican', 'VERB'),\n",
       " ('de', 'ADP'),\n",
       " ('dónde', 'ADV'),\n",
       " ('proviene', 'VERB'),\n",
       " ('el', 'DET'),\n",
       " ('agua', 'NOUN'),\n",
       " ('que', 'PRON'),\n",
       " ('bebió', 'VERB'),\n",
       " ('una', 'DET'),\n",
       " ('persona', 'NOUN'),\n",
       " ('durante', 'ADP'),\n",
       " ('su', 'DET'),\n",
       " ('niñez', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('y', 'CCONJ'),\n",
       " ('los', 'DET'),\n",
       " ('de', 'ADP'),\n",
       " ('las', 'DET'),\n",
       " ('mujeres', 'NOUN'),\n",
       " ('enterradas', 'ADJ'),\n",
       " ('en', 'ADP'),\n",
       " ('Gurgy', 'PRON'),\n",
       " ('son', 'AUX'),\n",
       " ('de', 'ADP'),\n",
       " ('muchos', 'DET'),\n",
       " ('lugares', 'NOUN'),\n",
       " ('distintos', 'ADJ'),\n",
       " ('.', 'PUNCT'),\n",
       " ('En', 'DET'),\n",
       " ('cambio', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('apenas', 'ADV'),\n",
       " ('hay', 'VERB'),\n",
       " ('mujeres', 'NOUN'),\n",
       " ('que', 'PRON'),\n",
       " ('fueran', 'AUX'),\n",
       " ('parte', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('los', 'DET'),\n",
       " ('dos', 'NUM'),\n",
       " ('clanes', 'NOUN'),\n",
       " ('originales', 'ADJ'),\n",
       " ('del', 'ADP'),\n",
       " ('lugar', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Estos', 'PRON'),\n",
       " ('hallazgos', 'AUX'),\n",
       " ('refuerzan', 'VERB'),\n",
       " ('una', 'DET'),\n",
       " ('tendencia', 'NOUN'),\n",
       " ('observada', 'ADJ'),\n",
       " ('en', 'ADP'),\n",
       " ('otros', 'DET'),\n",
       " ('yacimientos', 'NOUN'),\n",
       " ('neolíticos', 'CCONJ'),\n",
       " ('posteriores', 'ADJ'),\n",
       " (':', 'PUNCT'),\n",
       " ('los', 'DET'),\n",
       " ('hombres', 'NOUN'),\n",
       " ('se', 'PRON'),\n",
       " ('quedaban', 'VERB'),\n",
       " ('y', 'CCONJ'),\n",
       " ('las', 'DET'),\n",
       " ('mujeres', 'NOUN'),\n",
       " ('se', 'PRON'),\n",
       " ('marchaban', 'VERB'),\n",
       " ('a', 'ADP'),\n",
       " ('vivir', 'VERB'),\n",
       " ('y', 'CCONJ'),\n",
       " ('formar', 'VERB'),\n",
       " ('familias', 'NOUN'),\n",
       " ('en', 'ADP'),\n",
       " ('otros', 'DET'),\n",
       " ('grupos', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('una', 'DET'),\n",
       " ('práctica', 'NOUN'),\n",
       " ('común', 'ADJ'),\n",
       " ('en', 'ADP'),\n",
       " ('humanos', 'NOUN'),\n",
       " ('y', 'CCONJ'),\n",
       " ('otros', 'DET'),\n",
       " ('primates', 'NOUN'),\n",
       " ('conocida', 'ADJ'),\n",
       " ('como', 'SCONJ'),\n",
       " ('patrilocalidad', 'PUNCT'),\n",
       " ('y', 'CCONJ'),\n",
       " ('que', 'PRON'),\n",
       " ('evita', 'VERB'),\n",
       " ('los', 'DET'),\n",
       " ('problemas', 'NOUN'),\n",
       " ('asociados', '_'),\n",
       " ('a', 'ADP'),\n",
       " ('la', 'DET'),\n",
       " ('endogamia', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('La', 'PRON'),\n",
       " ('paleogenetista', 'VERB'),\n",
       " ('Maïte', 'DET'),\n",
       " ('Rivollat', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('primera', 'ADJ'),\n",
       " ('autora', 'NOUN'),\n",
       " ('del', '_'),\n",
       " ('estudio', 'VERB'),\n",
       " (',', 'PUNCT'),\n",
       " ('destaca', 'VERB'),\n",
       " ('otro', 'DET'),\n",
       " ('hallazgo', 'NOUN'),\n",
       " ('sorprendente', 'ADJ'),\n",
       " (':', 'PUNCT'),\n",
       " ('“', 'PRON'),\n",
       " ('Hemos', 'AUX'),\n",
       " ('encontrado', 'VERB'),\n",
       " ('parejas', 'SCONJ'),\n",
       " ('que', 'PRON'),\n",
       " ('tuvieron', 'VERB'),\n",
       " ('muchos', 'DET'),\n",
       " ('hijos', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('En', 'SCONJ'),\n",
       " ('un', 'DET'),\n",
       " ('caso', 'NOUN'),\n",
       " ('vemos', 'VERB'),\n",
       " ('hasta', 'ADP'),\n",
       " ('seis', 'NUM'),\n",
       " ('hermanos', 'NOUN'),\n",
       " ('que', 'PRON'),\n",
       " ('vivieron', 'VERB'),\n",
       " ('hasta', 'ADP'),\n",
       " ('la', 'DET'),\n",
       " ('edad', 'NOUN'),\n",
       " ('adulta', 'ADJ'),\n",
       " (',', 'PUNCT'),\n",
       " ('y', 'CCONJ'),\n",
       " ('a', 'ADP'),\n",
       " ('su', 'DET'),\n",
       " ('vez', 'NOUN'),\n",
       " ('tuvieron', 'VERB'),\n",
       " ('varios', 'DET'),\n",
       " ('hijos', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('lo', 'PRON'),\n",
       " ('que', 'PRON'),\n",
       " ('supone', 'VERB'),\n",
       " ('una', 'DET'),\n",
       " ('familia', 'NOUN'),\n",
       " ('muy', 'ADV'),\n",
       " ('extensa', 'ADJ'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Probablemente', 'PROPN'),\n",
       " (',', 'PUNCT'),\n",
       " ('tuvieron', 'VERB'),\n",
       " ('también', 'ADV'),\n",
       " ('hermanas', 'ADP'),\n",
       " ('cuyos', 'DET'),\n",
       " ('restos', 'NOUN'),\n",
       " ('no', 'ADV'),\n",
       " ('están', 'AUX'),\n",
       " ('aquí', 'ADV'),\n",
       " (',', 'PUNCT'),\n",
       " ('pues', 'SCONJ'),\n",
       " ('se', 'PRON'),\n",
       " ('fueron', 'AUX'),\n",
       " ('a', 'ADP'),\n",
       " ('vivir', 'VERB'),\n",
       " ('a', 'ADP'),\n",
       " ('otros', 'DET'),\n",
       " ('grupos', 'NOUN'),\n",
       " ('”', 'ADJ'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Para', 'SCONJ'),\n",
       " ('su', 'DET'),\n",
       " ('equipo', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('esto', 'PRON'),\n",
       " ('indica', 'VERB'),\n",
       " ('una', 'DET'),\n",
       " ('gran', 'ADJ'),\n",
       " ('fertilidad', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('las', 'DET'),\n",
       " ('mujeres', 'NOUN'),\n",
       " ('e', 'CCONJ'),\n",
       " ('implica', 'VERB'),\n",
       " ('que', 'PRON'),\n",
       " ('había', 'AUX'),\n",
       " ('abundancia', 'VERB'),\n",
       " ('de', 'ADP'),\n",
       " ('alimentos', 'NOUN'),\n",
       " ('y', 'CCONJ'),\n",
       " ('probablemente', 'ADV'),\n",
       " ('estabilidad', 'NOUN'),\n",
       " ('social', 'ADJ'),\n",
       " ('.', 'PUNCT'),\n",
       " ('De', 'DET'),\n",
       " ('hecho', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('no', 'ADV'),\n",
       " ('hay', 'VERB'),\n",
       " ('ni', 'CCONJ'),\n",
       " ('un', 'DET'),\n",
       " ('signo', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('violencia', 'NOUN'),\n",
       " ('en', 'ADP'),\n",
       " ('ninguno', 'PRON'),\n",
       " ('de', 'ADP'),\n",
       " ('los', 'DET'),\n",
       " ('más', 'ADV'),\n",
       " ('de', 'ADP'),\n",
       " ('100', 'NUM'),\n",
       " ('cadáveres', 'NOUN'),\n",
       " ('del', '_'),\n",
       " ('cementerio', 'VERB'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Apenas', 'SCONJ'),\n",
       " ('se', 'PRON'),\n",
       " ('han', 'AUX'),\n",
       " ('encontrado', 'VERB'),\n",
       " ('medio', 'NUM'),\n",
       " ('hermanos', 'NOUN'),\n",
       " ('en', 'ADP'),\n",
       " ('ninguno', 'PRON'),\n",
       " ('de', 'ADP'),\n",
       " ('los', 'DET'),\n",
       " ('dos', 'NUM'),\n",
       " ('clanes', 'NOUN'),\n",
       " ('familiares', 'ADJ'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Tampoco', 'SCONJ'),\n",
       " ('se', 'PRON'),\n",
       " ('observa', 'VERB'),\n",
       " ('que', 'SCONJ'),\n",
       " ('los', 'DET'),\n",
       " ('viudos', 'NOUN'),\n",
       " ('y', 'CCONJ'),\n",
       " ('las', 'DET'),\n",
       " ('viudas', 'NOUN'),\n",
       " ('se', 'PRON'),\n",
       " ('emparejasen', 'VERB'),\n",
       " ('con', 'ADP'),\n",
       " ('sus', 'DET'),\n",
       " ('cuñados', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Esto', 'PRON'),\n",
       " ('implica', 'VERB'),\n",
       " ('que', 'SCONJ'),\n",
       " ('las', 'DET'),\n",
       " ('parejas', 'NOUN'),\n",
       " ('eran', 'AUX'),\n",
       " ('monógamas', 'ADJ'),\n",
       " ('y', 'CCONJ'),\n",
       " ('que', 'PRON'),\n",
       " ('ya', 'ADV'),\n",
       " ('había', 'AUX'),\n",
       " ('una', 'DET'),\n",
       " ('idea', 'NOUN'),\n",
       " ('clara', 'ADJ'),\n",
       " ('de', 'ADP'),\n",
       " ('evitar', 'VERB'),\n",
       " ('tener', 'VERB'),\n",
       " ('hijos', 'NOUN'),\n",
       " ('con', 'ADP'),\n",
       " ('parientes', 'NOUN'),\n",
       " ('cercanos', 'ADJ'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('hmm_tagger_es.dill', 'rb') as file:\n",
    "    loaded_tagger = dill.load(file)\n",
    "\n",
    "loaded_tagger.tag(text_tokens_es)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKPreProcessing():\n",
    "    def __init__(self, text, lang) -> None:\n",
    "        self.text = text  \n",
    "        self.lang = lang\n",
    "\n",
    "    def remove_html_tags(self):\n",
    "        return self\n",
    "    \n",
    "    def to_lower(self):\n",
    "        self.text = self.text.lower()\n",
    "        return self\n",
    "\n",
    "    def remove_double_spaces(self):\n",
    "        self.text = [words for words in self.text if re.sub(' +', ' ', words)]\n",
    "        return self\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.text = word_tokenize(self.text)\n",
    "        return self\n",
    "    \n",
    "    def pos(self):\n",
    "        if self.lang == 'es':\n",
    "            self.text =  tagger_es.tag(self.text)\n",
    "        elif self.lang == 'en':\n",
    "            self.text =  pos_tag(self.text)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        if self.lang == 'es':\n",
    "            self.stopwd = stopwords.words('spanish')\n",
    "        elif self.lang == 'en':\n",
    "            self.stopwd = stopwords.words('english')\n",
    "        \n",
    "        self.text = [(word, tag) for word, tag in self.text if word not in self.stopwd]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_preprocessed(self):\n",
    "        return self.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class NLTKTextPreprocessor(TransformerMixin, BaseEstimator):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, text):\n",
    "    return self\n",
    "\n",
    "  def transform(self, text):\n",
    "    txt_preproc = NLTKPreProcessing(text, 'en')\n",
    "    processed_text = \\\n",
    "        txt_preproc \\\n",
    "        .remove_html_tags()\\\n",
    "        .to_lower()\\\n",
    "        .tokenize()\\\n",
    "        .remove_double_spaces()\\\n",
    "        .pos()\\\n",
    "        .remove_stopwords()\\\n",
    "        .get_preprocessed()\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elon', 'NN'),\n",
       " ('musk', 'NN'),\n",
       " ('said', 'VBD'),\n",
       " ('incident', 'NN'),\n",
       " ('wednesday', 'NN'),\n",
       " ('senate', 'JJ'),\n",
       " ('minority', 'NN'),\n",
       " ('leader', 'NN'),\n",
       " ('mitch', 'NN'),\n",
       " ('mcconnell', 'NN'),\n",
       " (',', ','),\n",
       " ('r-ky.', 'JJ'),\n",
       " (',', ','),\n",
       " ('appeared', 'VBD'),\n",
       " ('freeze', 'VB'),\n",
       " ('news', 'NN'),\n",
       " ('conference', 'NN'),\n",
       " ('republican', 'JJ'),\n",
       " ('leaders', 'NNS'),\n",
       " ('warrant', 'VB'),\n",
       " ('``', '``'),\n",
       " ('need', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('constitutional', 'JJ'),\n",
       " ('amendment.musk', 'NN'),\n",
       " (',', ','),\n",
       " ('owns', 'VBZ'),\n",
       " ('twitter', 'NN'),\n",
       " ('companies', 'NNS'),\n",
       " (',', ','),\n",
       " ('tweeted', 'VBD'),\n",
       " ('early', 'JJ'),\n",
       " ('thursday', 'NN'),\n",
       " ('morning', 'NN'),\n",
       " (',', ','),\n",
       " ('``', '``'),\n",
       " ('need', 'VBP'),\n",
       " ('constitutional', 'JJ'),\n",
       " ('amendment', 'NN'),\n",
       " ('.', '.'),\n",
       " ('insane', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('``', '``'),\n",
       " ('comment', 'NN'),\n",
       " ('came', 'VBD'),\n",
       " ('response', 'NN'),\n",
       " ('another', 'DT'),\n",
       " ('user', 'NN'),\n",
       " ('provided', 'VBD'),\n",
       " ('video', 'NN'),\n",
       " ('incident', 'NN'),\n",
       " ('showing', 'VBG'),\n",
       " ('mcconnell', 'NN'),\n",
       " ('addressing', 'VBG'),\n",
       " ('group', 'NN'),\n",
       " ('reporters', 'NNS'),\n",
       " ('abruptly', 'RB'),\n",
       " ('stopping', 'VBG'),\n",
       " (',', ','),\n",
       " ('staring', 'VBG'),\n",
       " ('blankly', 'RB'),\n",
       " ('group', 'NN'),\n",
       " (',', ','),\n",
       " ('standing', 'VBG'),\n",
       " ('idle', 'JJ'),\n",
       " ('brief', 'JJ'),\n",
       " ('moment', 'NN'),\n",
       " ('.', '.'),\n",
       " ('republican', 'JJ'),\n",
       " ('colleague', 'NN'),\n",
       " ('broke', 'VBD'),\n",
       " ('mcconnell', 'NNP'),\n",
       " ('’', 'NNP'),\n",
       " ('apparent', 'JJ'),\n",
       " ('trance', 'NN'),\n",
       " ('helped', 'VBD'),\n",
       " ('escort', 'VB'),\n",
       " ('away', 'RB'),\n",
       " ('podium.musk', 'NN'),\n",
       " ('’', 'NNP'),\n",
       " ('tweet', 'NN'),\n",
       " ('provide', 'VB'),\n",
       " ('specifics', 'NNS'),\n",
       " ('potential', 'JJ'),\n",
       " ('amendment', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('say', 'VB'),\n",
       " ('.', '.'),\n",
       " ('several', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('responded', 'VBD'),\n",
       " ('suggested', 'VBN'),\n",
       " ('age', 'NN'),\n",
       " ('limits', 'NNS'),\n",
       " ('members', 'NNS'),\n",
       " ('congress', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pure_transformation_pipeline = Pipeline(steps=[\n",
    "           ('text_preproc', NLTKTextPreprocessor())])\n",
    "tfidf_data = pure_transformation_pipeline.fit_transform(text_en)\n",
    "tfidf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 33.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from es-core-news-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.49.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/xhapa/anaconda3/envs/nlp-API/lib/python3.11/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKLemmatization():\n",
    "    def __init__(self, text, lang) -> None:\n",
    "        self.text = text\n",
    "        self.lang = lang\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    def get_wordnet_pos(self, tag):\n",
    "        tag = tag[0].upper()\n",
    "        tag_dict = {\n",
    "            'J': 'a',  # Adjective\n",
    "            'V': 'v',  # Verb\n",
    "            'N': 'n',  # Noun\n",
    "            'R': 'r'   # Adverb\n",
    "        }\n",
    "        return tag_dict.get(tag, 'n')\n",
    "    \n",
    "\n",
    "    def get_wordnet_upos(self, upos_tag):\n",
    "        tag_dict = {\n",
    "            'NOUN': 'n',  # Noun\n",
    "            'VERB': 'v',  # Verb\n",
    "            'ADJ': 'a',   # Adjective\n",
    "            'ADV': 'r'    # Adverb\n",
    "        }\n",
    "        return tag_dict.get(upos_tag, 'n')  \n",
    "\n",
    "    def lemmatize(self):\n",
    "        if self.lang == 'en':\n",
    "            self.text = [self.lemmatizer.lemmatize(word, pos=self.get_wordnet_pos(tag)) for word, tag in self.text]\n",
    "        elif self.lang == 'es':\n",
    "            words_with_space = [word + ' ' for word, tag in self.text]\n",
    "            doc = self.nlp(''.join(words_with_space))\n",
    "            self.text = [token.lemma_ for token in doc]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def get_lemmatization(self):\n",
    "        return  self.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKLemmatizationProcessor(TransformerMixin, BaseEstimator):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, text):\n",
    "    return self\n",
    "\n",
    "  def transform(self, text):\n",
    "    txt_lemma = NLTKLemmatization(text, 'en')\n",
    "    processed_text = \\\n",
    "        txt_lemma \\\n",
    "        .lemmatize()\\\n",
    "        .get_lemmatization()\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elon',\n",
       " 'musk',\n",
       " 'say',\n",
       " 'incident',\n",
       " 'wednesday',\n",
       " 'senate',\n",
       " 'minority',\n",
       " 'leader',\n",
       " 'mitch',\n",
       " 'mcconnell',\n",
       " ',',\n",
       " 'r-ky.',\n",
       " ',',\n",
       " 'appear',\n",
       " 'freeze',\n",
       " 'news',\n",
       " 'conference',\n",
       " 'republican',\n",
       " 'leader',\n",
       " 'warrant',\n",
       " '``',\n",
       " 'need',\n",
       " \"''\",\n",
       " 'constitutional',\n",
       " 'amendment.musk',\n",
       " ',',\n",
       " 'own',\n",
       " 'twitter',\n",
       " 'company',\n",
       " ',',\n",
       " 'tweet',\n",
       " 'early',\n",
       " 'thursday',\n",
       " 'morning',\n",
       " ',',\n",
       " '``',\n",
       " 'need',\n",
       " 'constitutional',\n",
       " 'amendment',\n",
       " '.',\n",
       " 'insane',\n",
       " '.',\n",
       " '``',\n",
       " 'comment',\n",
       " 'come',\n",
       " 'response',\n",
       " 'another',\n",
       " 'user',\n",
       " 'provide',\n",
       " 'video',\n",
       " 'incident',\n",
       " 'show',\n",
       " 'mcconnell',\n",
       " 'address',\n",
       " 'group',\n",
       " 'reporter',\n",
       " 'abruptly',\n",
       " 'stop',\n",
       " ',',\n",
       " 'star',\n",
       " 'blankly',\n",
       " 'group',\n",
       " ',',\n",
       " 'stand',\n",
       " 'idle',\n",
       " 'brief',\n",
       " 'moment',\n",
       " '.',\n",
       " 'republican',\n",
       " 'colleague',\n",
       " 'break',\n",
       " 'mcconnell',\n",
       " '’',\n",
       " 'apparent',\n",
       " 'trance',\n",
       " 'help',\n",
       " 'escort',\n",
       " 'away',\n",
       " 'podium.musk',\n",
       " '’',\n",
       " 'tweet',\n",
       " 'provide',\n",
       " 'specific',\n",
       " 'potential',\n",
       " 'amendment',\n",
       " 'would',\n",
       " 'say',\n",
       " '.',\n",
       " 'several',\n",
       " 'people',\n",
       " 'respond',\n",
       " 'suggest',\n",
       " 'age',\n",
       " 'limit',\n",
       " 'member',\n",
       " 'congress',\n",
       " '.']"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization_pipeline = Pipeline(steps=[\n",
    "           ('text_lemma', NLTKLemmatizationProcessor())])\n",
    "data = lemmatization_pipeline.fit_transform(tfidf_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords and Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mcconnell',\n",
       " '``',\n",
       " 'incident',\n",
       " 'republican',\n",
       " 'need',\n",
       " 'constitutional',\n",
       " 'amendment',\n",
       " 'group']"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "num_keywords = 10\n",
    "threshold = 1\n",
    "words = [word for word, tag in tfidf_data]\n",
    "fdist = FreqDist(words)\n",
    "keywords = [word for word, _ in fdist.most_common(num_keywords) if len(word) > threshold]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Elon', 'Musk'),\n",
       " ('Leader', 'Mitch'),\n",
       " ('Minority', 'Leader'),\n",
       " ('Musk', 'said'),\n",
       " ('Senate', 'Minority'),\n",
       " ('Several', 'people'),\n",
       " ('The', 'comment'),\n",
       " ('This', 'is'),\n",
       " ('Thursday', 'morning'),\n",
       " ('Wednesday', 'when')]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "bigrams_measure = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(text_tokens_en)\n",
    "finder.nbest(bigrams_measure.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Elon', 'Musk'),\n",
       " ('Leader', 'Mitch'),\n",
       " ('Minority', 'Leader'),\n",
       " ('Musk', 'said'),\n",
       " ('Senate', 'Minority'),\n",
       " ('Several', 'people'),\n",
       " ('The', 'comment'),\n",
       " ('Thursday', 'morning')]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwd = stopwords.words('english')\n",
    "[(word1, word2) for word1, word2 in finder.nbest(bigrams_measure.pmi, 10) if ((word1 not in stopwd) and (word2 not in stopwd))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "class NLTKEntityPreProcessing(NLTKPreProcessing):\n",
    "    def __init__(self, text, lang) -> None:\n",
    "        super().__init__(text, lang)\n",
    "\n",
    "    def get_continuous_chunks(self):\n",
    "        chunked = ne_chunk(self.text)\n",
    "        self.continuous_chunks = []\n",
    "        current_chunk = []\n",
    "        for i in chunked:\n",
    "                if type(i) == Tree:\n",
    "                        current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "                if current_chunk:\n",
    "                        named_entity = \" \".join(current_chunk)\n",
    "                        if named_entity not in self.continuous_chunks:\n",
    "                                self.continuous_chunks.append(named_entity)\n",
    "                                current_chunk = []\n",
    "                else:\n",
    "                        continue\n",
    "        return self\n",
    "\n",
    "    def get_preprocessed(self):\n",
    "        return self.continuous_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class NLTKEntityPreprocessor(TransformerMixin, BaseEstimator):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, text):\n",
    "    return self\n",
    "\n",
    "  def transform(self, text):\n",
    "    txt_preproc = NLTKEntityPreProcessing(text, 'en')\n",
    "    processed_text = \\\n",
    "        txt_preproc \\\n",
    "        .tokenize()\\\n",
    "        .pos()\\\n",
    "        .get_continuous_chunks()\\\n",
    "        .get_preprocessed()\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elon',\n",
       " 'Musk',\n",
       " 'Senate',\n",
       " 'Mitch McConnell',\n",
       " 'Republican',\n",
       " 'Twitter',\n",
       " 'McConnell',\n",
       " 'Republican McConnell',\n",
       " 'Congress']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "entity_transformation_pipeline = Pipeline(steps=[\n",
    "           ('text_preproc', NLTKEntityPreprocessor())])\n",
    "ent_data = entity_transformation_pipeline.fit_transform(text_en)\n",
    "ent_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment model classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastAPI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
